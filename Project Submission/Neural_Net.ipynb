{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification\n",
    "\n",
    "### CS 640 Project\n",
    "\n",
    "#### Ayush Shirsat, Sunitha Priyadarshini, Julie Park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.activations import relu, softmax, sigmoid\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disable warning message of tensorflow\n",
    "import numpy as np\n",
    "import csv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to train data\n",
    "path1 = './train_re/'\n",
    "X_data = []\n",
    "Y_data1 = []\n",
    "Y_data2 = []\n",
    "Y_data3 = []\n",
    "\n",
    "# load data\n",
    "with open('annot_train.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        X_data.append(row[0])\n",
    "        Y_data1.append(row[1])\n",
    "        Y_data2.append(row[2])\n",
    "        Y_data3.append(row[3:13])        \n",
    "csvFile.close()\n",
    "\n",
    "X_data = X_data[1:len(X_data)]\n",
    "\n",
    "X = np.array([np.array(cv2.imread(path1 + str(img),1)).flatten() for img in X_data],'f') \n",
    "X = X/255\n",
    "X = X.reshape(X.shape[0], 128, 128, 3)\n",
    "X = X.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Train Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data1 = np.array(Y_data1[1:len(Y_data1)])\n",
    "Y_data2 = np.array(Y_data2[1:len(Y_data2)])\n",
    "Y_data3 = np.array(Y_data3[1:len(Y_data3)])\n",
    "Y_data1 = Y_data1.astype('float')\n",
    "Y_data2 = Y_data2.astype('int')\n",
    "Y_data3 = Y_data3.astype('int')\n",
    "\n",
    "# Split data into validation and testing\n",
    "X_val = X[0:6000]\n",
    "X_train = X[6000:len(X)]\n",
    "\n",
    "Y_data1_val = Y_data1[0:6000]\n",
    "Y_data1 = Y_data1[6000:len(Y_data1)]\n",
    "\n",
    "Y_data2_val = Y_data2[0:6000]\n",
    "Y_data2 = Y_data2[6000:len(Y_data2)]\n",
    "\n",
    "Y_data3_val = Y_data3[0:6000]\n",
    "Y_data3 = Y_data3[6000:len(Y_data3)]\n",
    "\n",
    "X_train /= 255\n",
    "X_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ResNet-50 (add layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run either Resnet or VGG\n",
    "\n",
    "base_model=ResNet50(weights='imagenet',include_top=False, input_shape=(128, 128, 3)) # Uncomment For transfer learning\n",
    "# base_model=ResNet50(weights = None, include_top=False, input_shape=(128, 128, 3)) # Uncomment For non-tranfer learning\n",
    "\n",
    "# Added layers\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='sigmoid')(x) \n",
    "x=Dense(512,activation='sigmoid')(x) \n",
    "x=Dense(256,activation='sigmoid')(x) \n",
    "\n",
    "# output layers\n",
    "x1 = Dense(units=1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out1 = Activation(sigmoid)(x1)\n",
    "\n",
    "x2 = Dense(units=1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out2 = Activation(sigmoid)(x2)\n",
    "\n",
    "x3 = Dense(units=10, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out3 = Activation(sigmoid)(x3)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load VGG16 (add layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run either Resnet or VGG\n",
    "\n",
    "base_model=VGG16(weights='imagenet',include_top=False, input_shape=(128, 128, 3)) # Uncomment For transfer learning\n",
    "# base_model=VGG16(weights = None, include_top=False, input_shape=(128, 128, 3)) # Uncomment for non-tranfer learning\n",
    "\n",
    "# Added layers\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='sigmoid')(x) \n",
    "x=Dense(512,activation='sigmoid')(x) \n",
    "x=Dense(256,activation='sigmoid')(x) \n",
    "\n",
    "# output layers\n",
    "x1 = Dense(units=1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out1 = Activation(sigmoid)(x1)\n",
    "\n",
    "x2 = Dense(units=1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out2 = Activation(sigmoid)(x2)\n",
    "\n",
    "x3 = Dense(units=10, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out3 = Activation(sigmoid)(x3)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "model = Model(inputs=base_model.input, outputs=[out1,out2,out3])\n",
    "adam = Adam(lr = 0.001)\n",
    "model.compile(loss=['mean_squared_error','binary_crossentropy','binary_crossentropy'], optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Initialize callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=3, min_lr=0.00001)\n",
    "es = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Start training the model\n",
    "hist = model.fit(X_train, [Y_data1,Y_data2,Y_data3] , epochs=15, validation_data =(X_val, [Y_data1_val,Y_data2_val,Y_data3_val]) , batch_size=256, callbacks=[reduce_lr, es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Train & Validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss'] \n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(hist.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(hist.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(hist.history[\"val_loss\"]), np.min(hist.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "Y_test1 = []\n",
    "Y_test2 = []\n",
    "Y_test3 = []\n",
    "\n",
    "# Load test data\n",
    "with open('annot_test.csv', 'r') as csvFile:\n",
    "    reader2 = csv.reader(csvFile)\n",
    "    for row2 in reader2:\n",
    "        X_test.append(row2[0])\n",
    "        Y_test1.append(row2[1])\n",
    "        Y_test2.append(row2[2])\n",
    "        Y_test3.append(row2[3:13])\n",
    "csvFile.close()\n",
    "\n",
    "# Path for test data\n",
    "path2 = './test_re/'\n",
    "X_test = X_test[1:len(X_test)]\n",
    "\n",
    "Xte = np.array([np.array(cv2.imread(path2 + str(img2),1)).flatten() for img2 in X_test],'f') \n",
    "Xte = Xte/255\n",
    "Xte = Xte.reshape(Xte.shape[0], 128, 128, 3)\n",
    "Xte = Xte.astype('float')\n",
    "X_test = Xte\n",
    "\n",
    "Y_test1 = np.array(Y_test1[1:len(Y_test1)])\n",
    "Y_test2 = np.array(Y_test2[1:len(Y_test2)])\n",
    "Y_test3 = np.array(Y_test3[1:len(Y_test3)])\n",
    "Y_test1 = Y_test1.astype('float')\n",
    "Y_test2 = Y_test2.astype('int')\n",
    "Y_test3 = Y_test3.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test,[Y_test1,Y_test2,Y_test3])\n",
    "print(score[4:7])\n",
    "\n",
    "# i is what test sample to predict\n",
    "i=0\n",
    "temp = X_test[i].reshape(1,128,128,3)\n",
    "pred = model.predict(temp)\n",
    "print(Y_test1[i],Y_test2[i],Y_test3[i],'\\n', pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
