{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Input, Model\n",
    "from keras.layers import UpSampling2D, add\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, LeakyReLU, GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, rmsprop\n",
    "from keras.activations import relu, softmax, sigmoid\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Disable warning message of tensorflow\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#from sklearn.metrics import classification_report,confusion_matrix\n",
    "import csv\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load X data\n",
    "path1 = './train_64/'\n",
    "#X_data = os.listdir(path1)\n",
    "#X_data = sorted(X_data)\n",
    "X_data = []\n",
    "Y_data = []\n",
    "with open('annot_train.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        X_data.append(row[0])\n",
    "        Y_data.append(row[2])\n",
    "csvFile.close()\n",
    "\n",
    "X_data = X_data[1:len(X_data)]\n",
    "\n",
    "X = np.array([np.array(cv2.imread(path1 + str(img),0)).flatten() for img in X_data],'f') \n",
    "X = np.array(X)\n",
    "\n",
    "Y_data = Y_data[1:len(Y_data)]\n",
    "Y = np.array(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state = 4)\n",
    "X_train = X_train.reshape(X_train.shape[0], 64, 64, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 64, 64, 1)\n",
    "\n",
    "# Assigning X_train and X_val as float\n",
    "X_train = X_train.astype('float32') \n",
    "X_val = X_val.astype('float32')\n",
    "\n",
    "# Normalization of data \n",
    "# Data pixels are between 0 and 1\n",
    "X_train /= 255\n",
    "X_val /= 255\n",
    "\n",
    "Y_train = Y_train.astype('int64')\n",
    "Y_val = Y_val.astype('int64')\n",
    "# Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "# Y_val = np_utils.to_categorical(Y_val, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(n_output, upscale=False):\n",
    "    # n_output: number of feature maps in the block\n",
    "    # upscale: should we use the 1x1 conv2d mapping for shortcut or not\n",
    "    \n",
    "    # keras functional api: return the function of type\n",
    "    # Tensor -> Tensor\n",
    "    def f(x):\n",
    "        \n",
    "        # H_l(x):\n",
    "        # first pre-activation\n",
    "        h = BatchNormalization()(x)\n",
    "        h = Activation(sigmoid)(h)\n",
    "        # first convolution\n",
    "        h = Conv2D(kernel_size=3, filters=n_output, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(h)\n",
    "        \n",
    "        # second pre-activation\n",
    "        h = BatchNormalization()(x)\n",
    "        h = Activation(sigmoid)(h)\n",
    "        # second convolution\n",
    "        h = Conv2D(kernel_size=3, filters=n_output, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(h)\n",
    "        \n",
    "        # f(x):\n",
    "        if upscale:\n",
    "            # 1x1 conv2d\n",
    "            f = Conv2D(kernel_size=1, filters=n_output, strides=1, padding='same')(x)\n",
    "        else:\n",
    "            # identity\n",
    "            f = x\n",
    "        \n",
    "        # F_l(x) = f(x) + H_l(x):\n",
    "        return add([f, h])\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input((64, 64, 1))\n",
    "\n",
    "# first conv2d with post-activation to transform the input data to some reasonable form\n",
    "x = Conv2D(kernel_size=3, filters=8, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(input_tensor)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(sigmoid)(x)\n",
    "\n",
    "# F_1\n",
    "x = block(8)(x)\n",
    "# F_2\n",
    "x = block(8)(x)\n",
    "\n",
    "# F_3\n",
    "# H_3 is the function from the tensor of size 28x28x16 to the the tensor of size 28x28x32\n",
    "# and we can't add together tensors of inconsistent sizes, so we use upscale=True\n",
    "x = block(16, upscale=True)(x)       # !!! <------- Uncomment for local evaluation\n",
    "# F_4\n",
    "x = block(16)(x)                     # !!! <------- Uncomment for local evaluation\n",
    "# F_5\n",
    "x = block(16)(x)                     # !!! <------- Uncomment for local evaluation\n",
    "\n",
    "# F_6\n",
    "x = block(24, upscale=True)(x)       # !!! <------- Uncomment for local evaluation\n",
    "# F_7\n",
    "x = block(24)(x)                     # !!! <------- Uncomment for local evaluation\n",
    "\n",
    "# last activation of the entire network's output\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation(sigmoid)(x)\n",
    "\n",
    "# average pooling across the channels\n",
    "# 28x28x48 -> 1x48\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# dropout for more robust learning\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# last softmax layer\n",
    "x = Dense(units=1, kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "out1 = Activation(sigmoid)(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26088 samples, validate on 6523 samples\n",
      "Epoch 1/20\n",
      "26088/26088 [==============================] - 45s 2ms/step - loss: 1.7186 - acc: 0.5905 - val_loss: 1.6300 - val_acc: 0.7136\n",
      "Epoch 2/20\n",
      "26088/26088 [==============================] - 39s 1ms/step - loss: 1.6150 - acc: 0.7093 - val_loss: 1.5800 - val_acc: 0.7136\n",
      "Epoch 3/20\n",
      "26088/26088 [==============================] - 39s 1ms/step - loss: 1.5704 - acc: 0.7129 - val_loss: 1.5393 - val_acc: 0.7136\n",
      "Epoch 4/20\n",
      "26088/26088 [==============================] - 39s 1ms/step - loss: 1.5308 - acc: 0.7126 - val_loss: 1.5009 - val_acc: 0.7136\n",
      "Epoch 5/20\n",
      "22784/26088 [=========================>....] - ETA: 4s - loss: 1.4936 - acc: 0.7150"
     ]
    }
   ],
   "source": [
    "# Optimizer used is Stochastic Gradient Descent \n",
    "# Loss is calculated using categorical cross entropy\n",
    "model = Model(inputs=input_tensor, outputs=out1)\n",
    "model.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "\n",
    "def sigmoidal_decay(e, start=0, end=20, lr_start=0.001, lr_end=1e-5):\n",
    "    if e < start:\n",
    "        return lr_start\n",
    "    \n",
    "    if e > end:\n",
    "        return lr_end\n",
    "    \n",
    "    middle = (start + end) / 2\n",
    "    s = lambda x: 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    return s(13 * (-e + middle) / np.abs(end - start)) * np.abs(lr_start - lr_end) + lr_end\n",
    "\n",
    "lr = LearningRateScheduler(lambda e: sigmoidal_decay(e, end=100))\n",
    "\n",
    "results = model.fit(X_train, Y_train, epochs=20, validation_data=(X_val, Y_val), batch_size=256, callbacks=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing losses and accuracy\n",
    "train_loss = results.history['loss']\n",
    "val_loss = results.history['val_loss'] \n",
    "train_acc = results.history['acc']\n",
    "val_acc = results.history['val_acc']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(results.history[\"val_loss\"]), np.min(results.history[\"val_loss\"]), marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Load X data\n",
    "path1 = './test_64/'\n",
    "#X_data = os.listdir(path1)\n",
    "#X_data = sorted(X_data)\n",
    "X_data = []\n",
    "Y_data = []\n",
    "with open('annot_test.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    for row in reader:\n",
    "        X_data.append(row[0])\n",
    "        Y_data.append(row[1])\n",
    "csvFile.close()\n",
    "\n",
    "X_data = X_data[1:len(X_data)]\n",
    "\n",
    "X = np.array([np.array(cv2.imread(path1 + str(img),0)).flatten() for img in X_data],'f') \n",
    "X_test = np.array(X)\n",
    "\n",
    "Y_data = Y_data[1:len(Y_data)]\n",
    "Y_test = np.array(Y_data)\n",
    "\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 64, 64, 1)\n",
    "\n",
    "# Assigning X_train and X_test as float\n",
    "X_test = X_test.astype('float32') \n",
    "\n",
    "# Normalization of data \n",
    "# Data pixels are between 0 and 1\n",
    "X_test /= 255\n",
    "\n",
    "# Y_test = np_utils.to_categorical(Y_test, 2)\n",
    "\n",
    "Y_preds = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Y_preds,Y_test)\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "# print(score)\n",
    "pred = model.predict_classes(X_test[5])\n",
    "print(pred,np.argmax(Y_test[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
